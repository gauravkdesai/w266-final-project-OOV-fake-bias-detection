{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs Extensions\n",
    "\n",
    "### Objective: \n",
    "\n",
    "Determine if MIMICK model improve performance on classification task\n",
    "\n",
    "### Approach\n",
    "\n",
    "Create two models, each with an identical architecture but different embedding layer.  In the baseline we will use Google's word2vec embedding if it exists, and map everything else to a trainable <UNK> token.  In the second model, we will use Google's word2vec if it exists, the predicted embedding from MIMICK if the word fits our initial training criteria, and <UNK> otherwise.  \n",
    "    \n",
    "### Model\n",
    "\n",
    "We build two separate models, each of which are identical.  They consist of an embedding layer, a LSTM layer w/ attention, and finally a second LSTM layer.  The final state output from both models is concatenated and passed through a fully connected layer with Softmax activation.  We use categorical-crossentropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary for notebook\n",
    "\n",
    "import pandas as pd\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "from keras import optimizers, regularizers\n",
    "from keras.models import load_model, Model\n",
    "from keras.utils import CustomObjectScope, to_categorical\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Dropout, Embedding, Dense, LSTM, Bidirectional, Input, concatenate\n",
    "from keras.preprocessing import sequence \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# Set known dimension of word embeddings from word2vec\n",
    "H=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained word2vec embeddings and MIMICK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# Use gensim to load pre-trained google word2vec embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../word2vec_model/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "\n",
    "# Load our previously trained embedding model; we use our complex model\n",
    "lstm = load_model(\"../Saved_Models/mimic_model_complex_attention.h5\"\n",
    "                  , custom_objects=SeqSelfAttention.get_custom_objects())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate character index lookup which was used in MIMICK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_dicts(non_letter_chars, lower_case=True, upper_case=True):\n",
    "    \"\"\"\n",
    "    Create dictionary mapping characters to indices\n",
    "    :param non_letter_chars: list of characters which should be supported other than letters\n",
    "    :param lower_case: Should set of english lowercase letters be included; default True\n",
    "    :param upper_case: Should set of english uppercase letters be included; default True\n",
    "    \"\"\"\n",
    "    lower_case_letter_dict={}\n",
    "    upper_case_letter_dict={}\n",
    "    index_count = 0\n",
    "    # Create a dictionary with upper and lower case letters and associated index\n",
    "    # Note: We include underscores, hyphens, and apostrophes but ignore other characters\n",
    "    # found in word2vec model, including chinese symbols, emojis, etc\n",
    "    if lower_case:\n",
    "        lower_case_letter_dict = {letter: int(index)+index_count for index, letter in enumerate(ascii_lowercase, start=1)}\n",
    "        index_count += 26\n",
    "    if upper_case:\n",
    "        upper_case_letter_dict = {letter: int(index)+index_count for index, letter in enumerate(ascii_uppercase, start=1)} \n",
    "        index_count += 26\n",
    "        \n",
    "    chardict = {**lower_case_letter_dict, **upper_case_letter_dict}\n",
    "    \n",
    "    for char in non_letter_chars:\n",
    "        chardict[char] = index_count\n",
    "        index_count += 1\n",
    "\n",
    "    # Creation of reverse character lookup for debugging and word creation\n",
    "    reverse_chardict = {}\n",
    "    for k,v in chardict.items():\n",
    "        reverse_chardict[v] = k\n",
    "    \n",
    "    return chardict, reverse_chardict\n",
    "\n",
    "supported_non_letter_characters = ['-','\\'']\n",
    "chardict, reverse_chardict = create_char_dicts(supported_non_letter_characters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in quora data, and perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data using Pandas, and fill NaNs with blank text\n",
    "data = pd.read_csv(\"quora_train.csv\")\n",
    "data['question1'] = data['question1'].fillna('')\n",
    "data['question2'] = data['question2'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    \"\"\"\n",
    "    Helper function to take in string and return list\n",
    "    of words with key characters removed\n",
    "    \"\"\"\n",
    "    chars_to_remove=['?','!','_','(',')','[',']','..',':']\n",
    "    sc = set(chars_to_remove)\n",
    "    return_list = []\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        word = ''.join([c for c in word if c not in sc])\n",
    "        word = word.replace(\"/\",\" \")\n",
    "        word = word.replace(\"...\", \" \")\n",
    "        words=word.split()\n",
    "        for word in words:\n",
    "            return_list.append(word)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue preprocessing data by applying 'tokenize' function to all questions\n",
    "q1 = np.array(data['question1'].apply(tokenize))\n",
    "q2 = np.array(data['question2'].apply(tokenize))\n",
    "\n",
    "# Define our target outcome and convert to categorical response\n",
    "answers = np.array(data['is_duplicate'])\n",
    "answers = to_categorical(answers)\n",
    "\n",
    "# Create master list of all questions\n",
    "question_pairs = q1+q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word dictionary which will hold all the valid words we encounter\n",
    "\n",
    "word_dict = {}\n",
    "\n",
    "for question in question_pairs:\n",
    "    try:\n",
    "        for word in question:\n",
    "            word_dict[word] = 1\n",
    "    except:\n",
    "        print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipping and padding functions in order to prepare a word for MIMICK prediction\n",
    "def get_padding(sequence, max_padding = 25):\n",
    "    sequence = sequence[:max_padding] if len(sequence)>max_padding else sequence\n",
    "    result = np.zeros(max_padding)\n",
    "    result[max_padding-len(sequence):]=sequence\n",
    "    return result\n",
    "\n",
    "def get_padded_sequence(token, max_padding=25):\n",
    "    sequence = [chardict[c] if c in chardict else 0 for c in token]\n",
    "    return np.reshape(get_padding(sequence), (1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an embedding dictionary which has word and embedding.  \n",
    "Note: One dictionary will generate MIMICK predictions if the word is not avialable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 34005 tokens\n"
     ]
    }
   ],
   "source": [
    "# Loop through all known words and store word2vec embedding if it exists.  If it does not, and the\n",
    "# word meets the MIMICK training criteria then use the predicted embedding but only in emb_dict_model\n",
    "emb_dict_model = {}\n",
    "emb_dict_none = {}\n",
    "use_counter = 0\n",
    "words_needing_prediction = []\n",
    "\n",
    "for k,v in word_dict.items():\n",
    "    if model.vocab.get(k):\n",
    "        emb_dict_model[k] = model[k]\n",
    "        emb_dict_none[k] = model[k]\n",
    "    else:\n",
    "        if all(char in chardict.keys() for char in k) & (len(k)<=25):\n",
    "            use_counter += 1\n",
    "            words_needing_prediction.append(k)\n",
    "            emb_dict_model[k] = lstm.predict(get_padded_sequence(k))\n",
    "        \n",
    "print(\"Made predictions for %s tokens\" %use_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Create two tokenizers, one for our dictionary with MIMICK and one for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer based on our embedding dictionary, making sure we specify we want an OOV token\n",
    "tok_model = Tokenizer(num_words=len(emb_dict_model), oov_token='<UNK>')\n",
    "tok_model.fit_on_texts(emb_dict_model.keys())\n",
    "\n",
    "# Convert our questions into word sequences\n",
    "fq_seq_model = tok_model.texts_to_sequences(q1)\n",
    "sq_seq_model = tok_model.texts_to_sequences(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer based on our embedding dictionary, making sure we specify we want an OOV token\n",
    "tok = Tokenizer(num_words=len(emb_dict_none), oov_token='<UNK>')\n",
    "tok.fit_on_texts(emb_dict_none.keys())\n",
    "\n",
    "# Convert our questions into word sequences\n",
    "fq_seq_none = tok.texts_to_sequences(q1)\n",
    "sq_seq_none = tok.texts_to_sequences(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_question_length = 0\n",
    "\n",
    "for question in q1:\n",
    "    if len(question) > max_question_length:\n",
    "        max_question_length=len(question)\n",
    "        \n",
    "for question in q2:\n",
    "    if len(question) > max_question_length:\n",
    "        max_question_length=len(question)\n",
    "        \n",
    "fq_seq_model = sequence.pad_sequences(fq_seq_model, maxlen=max_question_length)\n",
    "sq_seq_model = sequence.pad_sequences(sq_seq_model, maxlen=max_question_length)\n",
    "fq_seq_none = sequence.pad_sequences(fq_seq_none, maxlen=max_question_length)\n",
    "sq_seq_none = sequence.pad_sequences(sq_seq_none, maxlen=max_question_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding layer which uses Google word2vec if known or Mimick if unknown\n",
    "word_index_model = tok_model.word_index\n",
    "\n",
    "embedding_matrix_model = np.zeros((len(word_index_model) + 1, H))\n",
    "for word, i in word_index_model.items():\n",
    "    embedding_vector = emb_dict_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_model[i] = embedding_vector\n",
    "        \n",
    "embedding_layer_model = Embedding(len(word_index_model) + 1,\n",
    "                            H,\n",
    "                            weights=[embedding_matrix_model],\n",
    "                            input_length=max_question_length,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create embedding layer which uses Google word2vec if known \n",
    "word_index = tok.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, H))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = emb_dict_none.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            H,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_question_length,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quora(embedding, path):\n",
    "    # Build Q1 stacked w/attention LSTM\n",
    "\n",
    "    model_1_input = Input(shape=(max_question_length,))\n",
    "    embeddings_1 = embedding(model_1_input)\n",
    "    model_1 = LSTM(50, return_sequences=True)(embeddings_1)\n",
    "    model_1 = SeqSelfAttention(attention_activation='sigmoid',\n",
    "                                                 kernel_regularizer=regularizers.l2(1e-5))(model_1)\n",
    "    model_1 = LSTM(50, return_sequences=False)(model_1)\n",
    "\n",
    "\n",
    "    # Build Q2 stacked w/attention LSTM\n",
    "    model_2_input = Input(shape=(max_question_length,))\n",
    "    embeddings_2 = embedding(model_2_input)\n",
    "    model_2 = LSTM(50, return_sequences=True)(embeddings_2)\n",
    "    model_2 = SeqSelfAttention(attention_activation='sigmoid',\n",
    "                                                 kernel_regularizer=regularizers.l2(1e-5))(model_2)\n",
    "    model_2 = LSTM(50, return_sequences=False)(model_2)\n",
    "\n",
    "    # Merge output of two models\n",
    "    joined = concatenate([model_1, model_2], axis = 1)\n",
    "\n",
    "    # Predict probabilities for our binary classification\n",
    "    outputs = Dense(2, activation='softmax')(joined)\n",
    "\n",
    "    # Define model as set of two inputs, and final output\n",
    "    final = Model(inputs=[model_1_input, model_2_input], output=outputs)\n",
    "\n",
    "    # Use cross-entropy as loss as this is a classification task\n",
    "    final.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # View the model so we can bask in Keras' glory!\n",
    "    final.summary()\n",
    "\n",
    "    # Fit our model on input questions and known target\n",
    "    final.fit([fq_seq_model, sq_seq_model], answers,\n",
    "              epochs=1, batch_size=250)\n",
    "\n",
    "    final.save(path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_95 (InputLayer)           (None, 237)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_96 (InputLayer)           (None, 237)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 237, 300)     24372000    input_95[0][0]                   \n",
      "                                                                 input_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_176 (LSTM)                 (None, 237, 50)      70200       embedding_1[90][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_178 (LSTM)                 (None, 237, 50)      70200       embedding_1[91][0]               \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_91 (SeqSelfA (None, 237, 50)      3265        lstm_176[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_92 (SeqSelfA (None, 237, 50)      3265        lstm_178[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_177 (LSTM)                 (None, 50)           20200       seq_self_attention_91[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_179 (LSTM)                 (None, 50)           20200       seq_self_attention_92[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 100)          0           lstm_177[0][0]                   \n",
      "                                                                 lstm_179[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 2)            202         concatenate_30[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 24,559,532\n",
      "Trainable params: 24,559,532\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "   250/404290 [..............................] - ETA: 15:26:51 - loss: 0.6974"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-e9371c2d2182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_quora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'quora_model_w_mimick.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-122-34773d42e03f>\u001b[0m in \u001b[0;36mtrain_quora\u001b[0;34m(embedding, path)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Fit our model on input questions and known target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     final.fit([fq_seq_model, sq_seq_model], answers,\n\u001b[0;32m---> 37\u001b[0;31m               epochs=1, batch_size=250)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_quora(embedding_layer_model,'quora_model_w_mimick.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_quora(embedding_layer,'quora_model_no_mimick.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
