{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../word2vec_model/GoogleNews-vectors-negative300.bin', binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_dicts(non_letter_chars, lower_case=True, upper_case=True):\n",
    "    \"\"\"\n",
    "    Create dictionary mapping characters to indices\n",
    "    :param non_letter_chars: list of characters which should be supported other than letters\n",
    "    :param lower_case: Should set of english lowercase letters be included; default True\n",
    "    :param upper_case: Should set of english uppercase letters be included; default True\n",
    "    \"\"\"\n",
    "    lower_case_letter_dict={}\n",
    "    upper_case_letter_dict={}\n",
    "    index_count = 0\n",
    "    # Create a dictionary with upper and lower case letters and associated index\n",
    "    # Note: We include underscores, hyphens, and apostrophes but ignore other characters\n",
    "    # found in word2vec model, including chinese symbols, emojis, etc\n",
    "    if lower_case:\n",
    "        lower_case_letter_dict = {letter: int(index)+index_count for index, letter in enumerate(ascii_lowercase, start=1)}\n",
    "        index_count += 26\n",
    "    if upper_case:\n",
    "        upper_case_letter_dict = {letter: int(index)+index_count for index, letter in enumerate(ascii_uppercase, start=1)} \n",
    "        index_count += 26\n",
    "        \n",
    "    chardict = {**lower_case_letter_dict, **upper_case_letter_dict}\n",
    "    \n",
    "    for char in non_letter_chars:\n",
    "        chardict[char] = index_count\n",
    "        index_count += 1\n",
    "\n",
    "    # Creation of reverse character lookup for debugging and word creation\n",
    "    reverse_chardict = {}\n",
    "    for k,v in chardict.items():\n",
    "        reverse_chardict[v] = k\n",
    "    \n",
    "    return chardict, reverse_chardict\n",
    "\n",
    "supported_non_letter_characters = ['-','\\'']\n",
    "chardict, reverse_chardict = create_char_dicts(supported_non_letter_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"quora_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question1'] = data['question1'].fillna('')\n",
    "data['question2'] = data['question2'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = list(data['question1'])+list(data['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "chars_to_remove=['?','!','_','(',')','[',']','..','...']\n",
    "sc = set(chars_to_remove)\n",
    "\n",
    "for question in all_questions:\n",
    "    try:\n",
    "        words = question.split()\n",
    "        for word in words:\n",
    "            word = ''.join([c for c in word if c not in sc])\n",
    "            word = word.replace(\"/\",\" \")\n",
    "            word_dict[word] = 1\n",
    "    except:\n",
    "        print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(word):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict_model = {}\n",
    "emb_dict_none = {}\n",
    "\n",
    "for k,v in word_dict.items():\n",
    "    if model.vocab.get(k):\n",
    "        emb_dict_model[k] = model[k]\n",
    "        emb_dict_none[k] = model[k]\n",
    "    else:\n",
    "        if all(char in chardict.keys() for char in k):\n",
    "            emb_dict_model[k] = predict_word(k)\n",
    "            emb_dict_none[k] = avg_embedding\n",
    "        else:\n",
    "            emb_dict_model[k] = avg_embedding\n",
    "            emb_dict_none[k] = avg_embedding\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding layer which uses Google word2vec if known or Mimick if unknown\n",
    "\n",
    "## Keras sample code for how to create embedding layer from pretrained; will repeat with each embedding dict from above\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding layer which uses Google word2vec if known, or overall avg. if unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Q1 LSTM\n",
    "\n",
    "model_1_input = Input()\n",
    "embeddings = model_1_input(embedding_layer)\n",
    "model_1 = LSTM()(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Q2 LSTM\n",
    "\n",
    "model_2_input = Input()\n",
    "embeddings = model_2_input(embedding_layer)\n",
    "model_2 = LSTM()(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LSTMS and concat.  Go through dense sigmoid to predict same or not same\n",
    "joined = keras.layers.Concatenate(axis=-1,[model_1,model_2])\n",
    "output = Dense()(joined)\n",
    "\n",
    "model = keras.models.Model(inputs=[model_1_input, model_2_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
