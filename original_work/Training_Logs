Original Model as Described in Paper
20d character embeddings, 1 biLSTM, 50 hidden units, no dropout, RMSE loss

WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 25, 20)            1120      
_________________________________________________________________
bidirectional_1 (Bidirection (None, 100)               28400     
_________________________________________________________________
dropout_1 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 300)               30300     
=================================================================
Total params: 59,820
Trainable params: 59,820
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/30
760310/760310 [==============================] - 121s 159us/step - loss: 0.0193
Epoch 2/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0190
Epoch 3/30
760310/760310 [==============================] - 116s 153us/step - loss: 0.0190
Epoch 4/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0189
Epoch 5/30
760310/760310 [==============================] - 120s 157us/step - loss: 0.0189
Epoch 6/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0188
Epoch 7/30
760310/760310 [==============================] - 117s 155us/step - loss: 0.0188
Epoch 8/30
760310/760310 [==============================] - 119s 156us/step - loss: 0.0187
Epoch 9/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0187
Epoch 10/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0187
Epoch 11/30
760310/760310 [==============================] - 116s 153us/step - loss: 0.0187
Epoch 12/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0187
Epoch 13/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0186
Epoch 14/30
760310/760310 [==============================] - 122s 161us/step - loss: 0.0186
Epoch 15/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0186
Epoch 16/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0186
Epoch 17/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0186
Epoch 18/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0186
Epoch 19/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0186
Epoch 20/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0186
Epoch 21/30
760310/760310 [==============================] - 118s 156us/step - loss: 0.0185
Epoch 22/30
760310/760310 [==============================] - 118s 155us/step - loss: 0.0185
Epoch 23/30
760310/760310 [==============================] - 119s 156us/step - loss: 0.0185
Epoch 24/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0185
Epoch 25/30
760310/760310 [==============================] - 117s 154us/step - loss: 0.0185
Epoch 26/30
760310/760310 [==============================] - 124s 163us/step - loss: 0.0185
Epoch 27/30
760310/760310 [==============================] - 116s 152us/step - loss: 0.0185
Epoch 28/30
760310/760310 [==============================] - 116s 152us/step - loss: 0.0185
Epoch 29/30
760310/760310 [==============================] - 116s 153us/step - loss: 0.0185
Epoch 30/30
760310/760310 [==============================] - 116s 153us/step - loss: 0.0185
Save Succesful

------------------------------------------------------------------------------------------------------------------------------

Original Model + Extra biLSTM layer
20d character embeddings, 50 hidden units, no dropout, RMSE loss, 2biLSTM layers, no attention

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 25, 20)            1120      
_________________________________________________________________
bidirectional_3 (Bidirection (None, 25, 100)           28400     
_________________________________________________________________
bidirectional_4 (Bidirection (None, 100)               60400     
_________________________________________________________________
dropout_2 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 300)               30300     
=================================================================
Total params: 120,220
Trainable params: 120,220
Non-trainable params: 0
_________________________________________________________________
Epoch 1/30
760310/760310 [==============================] - 301s 396us/step - loss: 0.0193
Epoch 2/30
760310/760310 [==============================] - 293s 385us/step - loss: 0.0190
Epoch 3/30
760310/760310 [==============================] - 303s 399us/step - loss: 0.0189
Epoch 4/30
760310/760310 [==============================] - 295s 389us/step - loss: 0.0188
Epoch 5/30
760310/760310 [==============================] - 298s 392us/step - loss: 0.0188
Epoch 6/30
760310/760310 [==============================] - 294s 387us/step - loss: 0.0187
Epoch 7/30
760310/760310 [==============================] - 292s 385us/step - loss: 0.0186
Epoch 8/30
760310/760310 [==============================] - 297s 391us/step - loss: 0.0186
Epoch 9/30
760310/760310 [==============================] - 290s 381us/step - loss: 0.0185
Epoch 10/30
760310/760310 [==============================] - 290s 381us/step - loss: 0.0185
Epoch 11/30
760310/760310 [==============================] - 289s 380us/step - loss: 0.0185
Epoch 12/30
760310/760310 [==============================] - 292s 384us/step - loss: 0.0184
Epoch 13/30
760310/760310 [==============================] - 296s 389us/step - loss: 0.0184
Epoch 14/30
760310/760310 [==============================] - 294s 387us/step - loss: 0.0184
Epoch 15/30
760310/760310 [==============================] - 293s 385us/step - loss: 0.0184
Epoch 16/30
760310/760310 [==============================] - 293s 385us/step - loss: 0.0183
Epoch 17/30
760310/760310 [==============================] - 291s 383us/step - loss: 0.0183
Epoch 18/30
760310/760310 [==============================] - 294s 387us/step - loss: 0.0183
Epoch 19/30
760310/760310 [==============================] - 289s 381us/step - loss: 0.0183
Epoch 20/30
760310/760310 [==============================] - 290s 382us/step - loss: 0.0183
Epoch 21/30
760310/760310 [==============================] - 291s 383us/step - loss: 0.0182
Epoch 22/30
760310/760310 [==============================] - 294s 387us/step - loss: 0.0182
Epoch 23/30
760310/760310 [==============================] - 291s 383us/step - loss: 0.0182
Epoch 24/30
760310/760310 [==============================] - 290s 382us/step - loss: 0.0182
Epoch 25/30
760310/760310 [==============================] - 289s 380us/step - loss: 0.0182
Epoch 26/30
760310/760310 [==============================] - 293s 386us/step - loss: 0.0182
Epoch 27/30
760310/760310 [==============================] - 290s 381us/step - loss: 0.0182
Epoch 28/30
760310/760310 [==============================] - 291s 382us/step - loss: 0.0182
Epoch 29/30
760310/760310 [==============================] - 289s 380us/step - loss: 0.0182
Epoch 30/30
760310/760310 [==============================] - 290s 381us/step - loss: 0.0181

------------------------------------------------------------------------------------------------------------------------------

Complicated Architecture Model
100d character embeddings, 2 biLSTM w/ attention and dropout; RMSE Loss; Dropouts for recurrent and dense = .05
Note: First epoch starts in the low .02s, with most of loss reduction occuring before first epoch ends (can't see that in Keras)

WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 25, 100)           5600      
_________________________________________________________________
bidirectional_1 (Bidirection (None, 25, 200)           160800    
_________________________________________________________________
seq_self_attention_1 (SeqSel (None, 25, 200)           12865     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 200)               240800    
_________________________________________________________________
dropout_1 (Dropout)          (None, 200)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 300)               60300     
=================================================================
Total params: 480,365
Trainable params: 480,365
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
Epoch 1/30
760310/760310 [==============================] - 882s 1ms/step - loss: 0.0193
Epoch 2/30
760310/760310 [==============================] - 868s 1ms/step - loss: 0.0190
Epoch 3/30
760310/760310 [==============================] - 871s 1ms/step - loss: 0.0189
Epoch 4/30
760310/760310 [==============================] - 867s 1ms/step - loss: 0.0187
Epoch 5/30
760310/760310 [==============================] - 868s 1ms/step - loss: 0.0186
Epoch 6/30
760310/760310 [==============================] - 870s 1ms/step - loss: 0.0185
Epoch 7/30
760310/760310 [==============================] - 879s 1ms/step - loss: 0.0184
Epoch 8/30
760310/760310 [==============================] - 876s 1ms/step - loss: 0.0184
Epoch 9/30
760310/760310 [==============================] - 872s 1ms/step - loss: 0.0183
Epoch 10/30
760310/760310 [==============================] - 876s 1ms/step - loss: 0.0182
Epoch 11/30
760310/760310 [==============================] - 877s 1ms/step - loss: 0.0182
Epoch 12/30
760310/760310 [==============================] - 874s 1ms/step - loss: 0.0182
Epoch 13/30
760310/760310 [==============================] - 875s 1ms/step - loss: 0.0181
Epoch 14/30
760310/760310 [==============================] - 872s 1ms/step - loss: 0.0181
Epoch 15/30
760310/760310 [==============================] - 875s 1ms/step - loss: 0.0181
Epoch 16/30
760310/760310 [==============================] - 878s 1ms/step - loss: 0.0180
Epoch 17/30
760310/760310 [==============================] - 875s 1ms/step - loss: 0.0180
Epoch 18/30
760310/760310 [==============================] - 875s 1ms/step - loss: 0.0180
Epoch 19/30
760310/760310 [==============================] - 876s 1ms/step - loss: 0.0180
Epoch 20/30
760310/760310 [==============================] - 874s 1ms/step - loss: 0.0179
Epoch 21/30
760310/760310 [==============================] - 873s 1ms/step - loss: 0.0179
Epoch 22/30
760310/760310 [==============================] - 875s 1ms/step - loss: 0.0179
Epoch 23/30
760310/760310 [==============================] - 871s 1ms/step - loss: 0.0179
Epoch 24/30
760310/760310 [==============================] - 867s 1ms/step - loss: 0.0179
Epoch 25/30
760310/760310 [==============================] - 875s 1ms/step - loss: 0.0179
Epoch 26/30
760310/760310 [==============================] - 879s 1ms/step - loss: 0.0178
Epoch 27/30
760310/760310 [==============================] - 866s 1ms/step - loss: 0.0178
Epoch 28/30
760310/760310 [==============================] - 868s 1ms/step - loss: 0.0178
Epoch 29/30
760310/760310 [==============================] - 868s 1ms/step - loss: 0.0178
Epoch 30/30
760310/760310 [==============================] - 867s 1ms/step - loss: 0.0178

----------------------------------------------------------------------------------------------------------------------

Complicated Architecture Model, Cosine Proximity as Loss Function
100d character embeddings, 2 biLSTM w/ attention and dropout; Cosine Loss; Dropouts for recurrent and dense = .05
Note: Majority of learning happens in first epoch

WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 25, 100)           5600      
_________________________________________________________________
bidirectional_1 (Bidirection (None, 25, 200)           160800    
_________________________________________________________________
seq_self_attention_1 (SeqSel (None, 25, 200)           12865     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 200)               240800    
_________________________________________________________________
dropout_1 (Dropout)          (None, 200)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 300)               60300     
=================================================================
Total params: 480,365
Trainable params: 480,365
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
Epoch 1/30
760310/760310 [==============================] - 897s 1ms/step - loss: -0.3729
Epoch 2/30
760310/760310 [==============================] - 873s 1ms/step - loss: -0.4002
Epoch 3/30
760310/760310 [==============================] - 874s 1ms/step - loss: -0.4125
Epoch 4/30
760310/760310 [==============================] - 871s 1ms/step - loss: -0.4202
Epoch 5/30
760310/760310 [==============================] - 872s 1ms/step - loss: -0.4256
Epoch 6/30
760310/760310 [==============================] - 873s 1ms/step - loss: -0.4296
Epoch 7/30
760310/760310 [==============================] - 880s 1ms/step - loss: -0.4328
Epoch 8/30
760310/760310 [==============================] - 872s 1ms/step - loss: -0.4355
Epoch 9/30
760310/760310 [==============================] - 874s 1ms/step - loss: -0.4377
Epoch 10/30
760310/760310 [==============================] - 875s 1ms/step - loss: -0.4397
Epoch 11/30
760310/760310 [==============================] - 878s 1ms/step - loss: -0.4414
Epoch 12/30
760310/760310 [==============================] - 872s 1ms/step - loss: -0.4429
Epoch 13/30
760310/760310 [==============================] - 878s 1ms/step - loss: -0.4443
Epoch 14/30
760310/760310 [==============================] - 876s 1ms/step - loss: -0.4456
Epoch 15/30
760310/760310 [==============================] - 873s 1ms/step - loss: -0.4467
Epoch 16/30
760310/760310 [==============================] - 867s 1ms/step - loss: -0.4477
Epoch 17/30
760310/760310 [==============================] - 875s 1ms/step - loss: -0.4487
Epoch 18/30
760310/760310 [==============================] - 871s 1ms/step - loss: -0.4495
Epoch 19/30
760310/760310 [==============================] - 864s 1ms/step - loss: -0.4504
Epoch 20/30
760310/760310 [==============================] - 870s 1ms/step - loss: -0.4511
Epoch 21/30
760310/760310 [==============================] - 869s 1ms/step - loss: -0.4519
Epoch 22/30
760310/760310 [==============================] - 869s 1ms/step - loss: -0.4525
Epoch 23/30
760310/760310 [==============================] - 863s 1ms/step - loss: -0.4532
Epoch 24/30
760310/760310 [==============================] - 869s 1ms/step - loss: -0.4538
Epoch 25/30
760310/760310 [==============================] - 869s 1ms/step - loss: -0.4544
Epoch 26/30
760310/760310 [==============================] - 866s 1ms/step - loss: -0.4549
Epoch 27/30
760310/760310 [==============================] - 866s 1ms/step - loss: -0.4555
Epoch 28/30
760310/760310 [==============================] - 869s 1ms/step - loss: -0.4560
Epoch 29/30
760310/760310 [==============================] - 867s 1ms/step - loss: -0.4564
Epoch 30/30
760310/760310 [==============================] - 862s 1ms/step - loss: -0.4569
Save Succesful