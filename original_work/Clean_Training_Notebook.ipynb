{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things left to do on model front\n",
    "\n",
    "    1) Either use original polyglot embeddings or original code on word2vec to make sure our model is getting similar results; would be really dumb if we have a small mistake in our model and aren't actually training it correctly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful library for embeddings\n",
    "\n",
    "import class_lstm\n",
    "from class_lstm import MimicLSTM\n",
    "\n",
    "import word2vec_preprocess\n",
    "import word2vec_preprocess as preprocess\n",
    "\n",
    "import gensim\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing import sequence \n",
    "import tensorflow as tf\n",
    "\n",
    "import gensim\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './word2vec_model/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "data_dict, chardict = preprocess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_word(word, word_emb_dict, lstm_model, char_dict, word2vec):\n",
    "    indices = np.array([char_dict[letter] for letter in word])\n",
    "    indices = sequence.pad_sequences([indices], maxlen=25) \n",
    "    prediction = lstm_model.predict(tf.reshape(indices,(1,-1)), steps=1)\n",
    "    \n",
    "    if word in word2vec:\n",
    "        actual = np.reshape(word2vec[word],(1,-1))\n",
    "        actual_sim = cosine_similarity(actual,prediction)\n",
    "        print(\"Actual sim is %s\" %actual_sim)\n",
    "    else:\n",
    "        actual_sim = \"Not Known\"\n",
    "    \n",
    "    best_word = ''\n",
    "    best_sim = 0\n",
    "    total_sim = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for k,v in word_emb_dict.items():\n",
    "        \n",
    "        v = np.reshape(v,(1,-1))\n",
    "        sim = cosine_similarity(prediction, v)\n",
    "        total_sim += sim\n",
    "        total_count += 1\n",
    "        \n",
    "        if sim > best_sim:\n",
    "            best_word = k\n",
    "            best_sim = sim\n",
    "            print(best_word, best_sim)\n",
    "    avg_sim = total_sim/total_count\n",
    "    \n",
    "    print(\"actual sim was %s while avg was %s\" %(actual_sim,avg_sim))\n",
    "    return actual_sim, avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'word2vec_preprocess' from '/home/cgleach/w266/final_project/w266-final-project-OOV-fake-bias-detection/original_work/word2vec_preprocess.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload our packages as we test \n",
    "importlib.reload(class_lstm)\n",
    "importlib.reload(word2vec_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 20)            1120      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               28400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 59,820\n",
      "Trainable params: 59,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "760310/760310 [==============================] - 121s 159us/step - loss: 0.0193\n",
      "Epoch 2/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0190\n",
      "Epoch 3/30\n",
      "760310/760310 [==============================] - 116s 153us/step - loss: 0.0190\n",
      "Epoch 4/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0189\n",
      "Epoch 5/30\n",
      "760310/760310 [==============================] - 120s 157us/step - loss: 0.0189\n",
      "Epoch 6/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0188\n",
      "Epoch 7/30\n",
      "760310/760310 [==============================] - 117s 155us/step - loss: 0.0188\n",
      "Epoch 8/30\n",
      "760310/760310 [==============================] - 119s 156us/step - loss: 0.0187\n",
      "Epoch 9/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0187\n",
      "Epoch 10/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0187\n",
      "Epoch 11/30\n",
      "760310/760310 [==============================] - 116s 153us/step - loss: 0.0187\n",
      "Epoch 12/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0187\n",
      "Epoch 13/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0186\n",
      "Epoch 14/30\n",
      "760310/760310 [==============================] - 122s 161us/step - loss: 0.0186\n",
      "Epoch 15/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0186\n",
      "Epoch 16/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0186\n",
      "Epoch 17/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0186\n",
      "Epoch 18/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0186\n",
      "Epoch 19/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0186\n",
      "Epoch 20/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0186\n",
      "Epoch 21/30\n",
      "760310/760310 [==============================] - 118s 156us/step - loss: 0.0185\n",
      "Epoch 22/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0185\n",
      "Epoch 23/30\n",
      "760310/760310 [==============================] - 119s 156us/step - loss: 0.0185\n",
      "Epoch 24/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0185\n",
      "Epoch 25/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0185\n",
      "Epoch 26/30\n",
      "760310/760310 [==============================] - 124s 163us/step - loss: 0.0185\n",
      "Epoch 27/30\n",
      "760310/760310 [==============================] - 116s 152us/step - loss: 0.0185\n",
      "Epoch 28/30\n",
      "760310/760310 [==============================] - 116s 152us/step - loss: 0.0185\n",
      "Epoch 29/30\n",
      "760310/760310 [==============================] - 116s 153us/step - loss: 0.0185\n",
      "Epoch 30/30\n",
      "760310/760310 [==============================] - 116s 153us/step - loss: 0.0185\n",
      "Save Succesful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class_lstm.MimicLSTM at 0x7f21307b7da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original model as outlined in paper\n",
    "layers = 1\n",
    "H = 50\n",
    "character_dim = 20\n",
    "output_dim = 300\n",
    "\n",
    "model = MimicLSTM(layers, H, chardict, character_dim, data_dict, \n",
    "                  batch_size=1000, epochs=30, full_set=True)\n",
    "\n",
    "model.save_model(\"./mimic_model_original.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Succesful\n"
     ]
    }
   ],
   "source": [
    "layers = 1\n",
    "H = 50\n",
    "character_dim = 20\n",
    "output_dim = 300\n",
    "\n",
    "loadmodel = MimicLSTM(layers, H, chardict, character_dim, data_dict, \n",
    "                  train=False, load_path=\"./mimic_model_original.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 20)            1120      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 25, 100)           28400     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 120,220\n",
      "Trainable params: 120,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "760310/760310 [==============================] - 301s 396us/step - loss: 0.0193\n",
      "Epoch 2/30\n",
      "760310/760310 [==============================] - 293s 385us/step - loss: 0.0190\n",
      "Epoch 3/30\n",
      "760310/760310 [==============================] - 303s 399us/step - loss: 0.0189\n",
      "Epoch 4/30\n",
      "760310/760310 [==============================] - 295s 389us/step - loss: 0.0188\n",
      "Epoch 5/30\n",
      "760310/760310 [==============================] - 298s 392us/step - loss: 0.0188\n",
      "Epoch 6/30\n",
      "760310/760310 [==============================] - 294s 387us/step - loss: 0.0187\n",
      "Epoch 7/30\n",
      "760310/760310 [==============================] - 292s 385us/step - loss: 0.0186\n",
      "Epoch 8/30\n",
      "760310/760310 [==============================] - 297s 391us/step - loss: 0.0186\n",
      "Epoch 9/30\n",
      "760310/760310 [==============================] - 290s 381us/step - loss: 0.0185\n",
      "Epoch 10/30\n",
      "760310/760310 [==============================] - 290s 381us/step - loss: 0.0185\n",
      "Epoch 11/30\n",
      "760310/760310 [==============================] - 289s 380us/step - loss: 0.0185\n",
      "Epoch 12/30\n",
      "760310/760310 [==============================] - 292s 384us/step - loss: 0.0184\n",
      "Epoch 13/30\n",
      "760310/760310 [==============================] - 296s 389us/step - loss: 0.0184\n",
      "Epoch 14/30\n",
      "760310/760310 [==============================] - 294s 387us/step - loss: 0.0184\n",
      "Epoch 15/30\n",
      "760310/760310 [==============================] - 293s 385us/step - loss: 0.0184\n",
      "Epoch 16/30\n",
      "760310/760310 [==============================] - 293s 385us/step - loss: 0.0183\n",
      "Epoch 17/30\n",
      "760310/760310 [==============================] - 291s 383us/step - loss: 0.0183\n",
      "Epoch 18/30\n",
      "760310/760310 [==============================] - 294s 387us/step - loss: 0.0183\n",
      "Epoch 19/30\n",
      "760310/760310 [==============================] - 289s 381us/step - loss: 0.0183\n",
      "Epoch 20/30\n",
      "760310/760310 [==============================] - 290s 382us/step - loss: 0.0183\n",
      "Epoch 21/30\n",
      "760310/760310 [==============================] - 291s 383us/step - loss: 0.0182\n",
      "Epoch 22/30\n",
      "760310/760310 [==============================] - 294s 387us/step - loss: 0.0182\n",
      "Epoch 23/30\n",
      "760310/760310 [==============================] - 291s 383us/step - loss: 0.0182\n",
      "Epoch 24/30\n",
      "760310/760310 [==============================] - 290s 382us/step - loss: 0.0182\n",
      "Epoch 25/30\n",
      "760310/760310 [==============================] - 289s 380us/step - loss: 0.0182\n",
      "Epoch 26/30\n",
      "760310/760310 [==============================] - 293s 386us/step - loss: 0.0182\n",
      "Epoch 27/30\n",
      "760310/760310 [==============================] - 290s 381us/step - loss: 0.0182\n",
      "Epoch 28/30\n",
      "760310/760310 [==============================] - 291s 382us/step - loss: 0.0182\n",
      "Epoch 29/30\n",
      "760310/760310 [==============================] - 289s 380us/step - loss: 0.0182\n",
      "Epoch 30/30\n",
      "760310/760310 [==============================] - 290s 381us/step - loss: 0.0181\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c9139fcc37bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                           epochs = 30, full_set = True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./mimic_model_complex.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Double the layers\n",
    "layers = 2\n",
    "H = 50\n",
    "character_dim = 20\n",
    "output_dim = 300\n",
    "\n",
    "model_complex = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 30, full_set = True)\n",
    "\n",
    "model_complex.save_model(\"./mimic_model_complex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Succesful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class_lstm.MimicLSTM at 0x7fb0c4534da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_complex.save_model(\"./mimic_model_complex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 100)           5600      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 200)           160800    \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, 25, 200)           12865     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 480,365\n",
      "Trainable params: 480,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/30\n",
      "760310/760310 [==============================] - 882s 1ms/step - loss: 0.0193\n",
      "Epoch 2/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0190\n",
      "Epoch 3/30\n",
      "760310/760310 [==============================] - 871s 1ms/step - loss: 0.0189\n",
      "Epoch 4/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: 0.0187\n",
      "Epoch 5/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0186\n",
      "Epoch 6/30\n",
      "760310/760310 [==============================] - 870s 1ms/step - loss: 0.0185\n",
      "Epoch 7/30\n",
      "760310/760310 [==============================] - 879s 1ms/step - loss: 0.0184\n",
      "Epoch 8/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: 0.0184\n",
      "Epoch 9/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: 0.0183\n",
      "Epoch 10/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: 0.0182\n",
      "Epoch 11/30\n",
      "760310/760310 [==============================] - 877s 1ms/step - loss: 0.0182\n",
      "Epoch 12/30\n",
      "760310/760310 [==============================] - 874s 1ms/step - loss: 0.0182\n",
      "Epoch 13/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0181\n",
      "Epoch 14/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: 0.0181\n",
      "Epoch 15/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0181\n",
      "Epoch 16/30\n",
      "760310/760310 [==============================] - 878s 1ms/step - loss: 0.0180\n",
      "Epoch 17/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0180\n",
      "Epoch 18/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0180\n",
      "Epoch 19/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: 0.0180\n",
      "Epoch 20/30\n",
      "760310/760310 [==============================] - 874s 1ms/step - loss: 0.0179\n",
      "Epoch 21/30\n",
      "760310/760310 [==============================] - 873s 1ms/step - loss: 0.0179\n",
      "Epoch 22/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0179\n",
      "Epoch 23/30\n",
      "760310/760310 [==============================] - 871s 1ms/step - loss: 0.0179\n",
      "Epoch 24/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: 0.0179\n",
      "Epoch 25/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0179\n",
      "Epoch 26/30\n",
      "760310/760310 [==============================] - 879s 1ms/step - loss: 0.0178\n",
      "Epoch 27/30\n",
      "760310/760310 [==============================] - 866s 1ms/step - loss: 0.0178\n",
      "Epoch 28/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0178\n",
      "Epoch 29/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0178\n",
      "Epoch 30/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "# Super complicated model\n",
    "\n",
    "layers = 2\n",
    "H = 100\n",
    "character_dim = 100\n",
    "output_dim = 300\n",
    "recurrent_dropout = .05\n",
    "dense_dropout = .05\n",
    "attention = True\n",
    "\n",
    "model_complex_attention = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 30, full_set = True, use_attention=attention, \n",
    "                        recurrent_dropout = recurrent_dropout, dense_dropout = dense_dropout)\n",
    "\n",
    "#model_complex_attention.save_model(\"./mimic_model_complex_attention.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 100)           5600      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 200)           160800    \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, 25, 200)           12865     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 480,365\n",
      "Trainable params: 480,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/30\n",
      "760310/760310 [==============================] - 897s 1ms/step - loss: -0.3729\n",
      "Epoch 2/30\n",
      "760310/760310 [==============================] - 873s 1ms/step - loss: -0.4002\n",
      "Epoch 3/30\n",
      "760310/760310 [==============================] - 874s 1ms/step - loss: -0.4125\n",
      "Epoch 4/30\n",
      "760310/760310 [==============================] - 871s 1ms/step - loss: -0.4202\n",
      "Epoch 5/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: -0.4256\n",
      "Epoch 6/30\n",
      "760310/760310 [==============================] - 873s 1ms/step - loss: -0.4296\n",
      "Epoch 7/30\n",
      "760310/760310 [==============================] - 880s 1ms/step - loss: -0.4328\n",
      "Epoch 8/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: -0.4355\n",
      "Epoch 9/30\n",
      "760310/760310 [==============================] - 874s 1ms/step - loss: -0.4377\n",
      "Epoch 10/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: -0.4397\n",
      "Epoch 11/30\n",
      "760310/760310 [==============================] - 878s 1ms/step - loss: -0.4414\n",
      "Epoch 12/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: -0.4429\n",
      "Epoch 13/30\n",
      "760310/760310 [==============================] - 878s 1ms/step - loss: -0.4443\n",
      "Epoch 14/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: -0.4456\n",
      "Epoch 15/30\n",
      "760310/760310 [==============================] - 873s 1ms/step - loss: -0.4467\n",
      "Epoch 16/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: -0.4477\n",
      "Epoch 17/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: -0.4487\n",
      "Epoch 18/30\n",
      "760310/760310 [==============================] - 871s 1ms/step - loss: -0.4495\n",
      "Epoch 19/30\n",
      "760310/760310 [==============================] - 864s 1ms/step - loss: -0.4504\n",
      "Epoch 20/30\n",
      "760310/760310 [==============================] - 870s 1ms/step - loss: -0.4511\n",
      "Epoch 21/30\n",
      "760310/760310 [==============================] - 869s 1ms/step - loss: -0.4519\n",
      "Epoch 22/30\n",
      "760310/760310 [==============================] - 869s 1ms/step - loss: -0.4525\n",
      "Epoch 23/30\n",
      "760310/760310 [==============================] - 863s 1ms/step - loss: -0.4532\n",
      "Epoch 24/30\n",
      "760310/760310 [==============================] - 869s 1ms/step - loss: -0.4538\n",
      "Epoch 25/30\n",
      "760310/760310 [==============================] - 869s 1ms/step - loss: -0.4544\n",
      "Epoch 26/30\n",
      "760310/760310 [==============================] - 866s 1ms/step - loss: -0.4549\n",
      "Epoch 27/30\n",
      "760310/760310 [==============================] - 866s 1ms/step - loss: -0.4555\n",
      "Epoch 28/30\n",
      "760310/760310 [==============================] - 869s 1ms/step - loss: -0.4560\n",
      "Epoch 29/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: -0.4564\n",
      "Epoch 30/30\n",
      "760310/760310 [==============================] - 862s 1ms/step - loss: -0.4569\n",
      "Save Succesful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class_lstm.MimicLSTM at 0x7fcb13a14fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = 2\n",
    "H = 100\n",
    "character_dim = 100\n",
    "output_dim = 300\n",
    "recurrent_dropout = .05\n",
    "dense_dropout = .05\n",
    "attention = True\n",
    "\n",
    "model_complex_attention_cosine = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 30, full_set = True, use_attention=attention, loss_function = 'cosine_proximity', \n",
    "                        recurrent_dropout = recurrent_dropout, dense_dropout = dense_dropout)\n",
    "\n",
    "model_complex_attention_cosine.save_model(\"./mimic_model_complex_attention_cosine_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
