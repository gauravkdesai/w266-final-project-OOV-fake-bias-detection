{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things left to do on model front\n",
    "\n",
    "    1) Either use original polyglot embeddings or original code on word2vec to make sure our model is getting similar results; would be really dumb if we have a small mistake in our model and aren't actually training it correctly\n",
    "\n",
    "    2) Build 1-2 more models of increasing complexity.  Perhaps we add attention to one of the biLSTM layers or attempt encoder decoder approach.\n",
    "    \n",
    "    3) Build script that takes a feature and works through getting avg. embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Useful library for embeddings\n",
    "\n",
    "import class_lstm\n",
    "from class_lstm import MimicLSTM\n",
    "\n",
    "import word2vec_preprocess\n",
    "import word2vec_preprocess as preprocess\n",
    "\n",
    "import gensim\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing import sequence \n",
    "import tensorflow as tf\n",
    "\n",
    "import gensim\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './word2vec_model/GoogleNews-vectors-negative300.bin', binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_word(word, word_emb_dict, lstm_model, char_dict, word2vec):\n",
    "    indices = np.array([char_dict[letter] for letter in word])\n",
    "    indices = sequence.pad_sequences([indices], maxlen=25) \n",
    "    prediction = lstm_model.predict(tf.reshape(indices,(1,-1)), steps=1)\n",
    "    \n",
    "    actual = np.reshape(word2vec[word],(1,-1))\n",
    "    actual_sim = cosine_similarity(actual,prediction)\n",
    "    print(\"Actual sim is %s\" %actual_sim)\n",
    "    \n",
    "    best_word = ''\n",
    "    best_sim = 0\n",
    "    total_sim = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for k,v in word_emb_dict.items():\n",
    "        \n",
    "        v = np.reshape(v,(1,-1))\n",
    "        sim = cosine_similarity(prediction, v)\n",
    "        total_sim += sim\n",
    "        total_count += 1\n",
    "        \n",
    "        if sim > best_sim:\n",
    "            best_word = k\n",
    "            best_sim = sim\n",
    "            print(best_word, best_sim)\n",
    "    avg_sim = total_sim/total_count\n",
    "    \n",
    "    print(\"actual sim was %s while avg was %s\" %(actual_sim,avg_sim))\n",
    "    return actual_sim, avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, chardict = preprocess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 1\n",
    "H = 50\n",
    "character_dim = 20\n",
    "output_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'word2vec_preprocess' from '/home/cgleach/w266/final_project/w266-final-project-OOV-fake-bias-detection/original_work/word2vec_preprocess.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload our packages as we test \n",
    "importlib.reload(class_lstm)\n",
    "importlib.reload(word2vec_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 20)            1120      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100)               28400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 59,820\n",
      "Trainable params: 59,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "760310/760310 [==============================] - 119s 157us/sample - loss: 0.0193\n",
      "Epoch 2/10\n",
      "760310/760310 [==============================] - 117s 154us/sample - loss: 0.0190\n",
      "Epoch 3/10\n",
      "760310/760310 [==============================] - 117s 154us/sample - loss: 0.0190\n",
      "Epoch 4/10\n",
      "760310/760310 [==============================] - 118s 155us/sample - loss: 0.0189\n",
      "Epoch 5/10\n",
      "760310/760310 [==============================] - 118s 156us/sample - loss: 0.0189\n",
      "Epoch 6/10\n",
      "760310/760310 [==============================] - 117s 153us/sample - loss: 0.0188\n",
      "Epoch 7/10\n",
      "760310/760310 [==============================] - 118s 155us/sample - loss: 0.0188\n",
      "Epoch 8/10\n",
      "760310/760310 [==============================] - 122s 160us/sample - loss: 0.0188\n",
      "Epoch 9/10\n",
      "760310/760310 [==============================] - 118s 155us/sample - loss: 0.0188\n",
      "Epoch 10/10\n",
      "760310/760310 [==============================] - 117s 154us/sample - loss: 0.0187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Save Succesful'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MimicLSTM(layers, H, chardict, character_dim, data_dict, \n",
    "                  batch_size=1000, epochs=10, full_set=True)\n",
    "\n",
    "model.save_model(\"./mimic_model_original.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sim is [[0.29064178]]\n",
      "in [[0.08038858]]\n",
      "that [[0.2208077]]\n",
      "or [[0.23610187]]\n",
      "government [[0.24608949]]\n",
      "such [[0.2760615]]\n",
      "these [[0.30484337]]\n",
      "products [[0.34159136]]\n",
      "technology [[0.34362707]]\n",
      "product [[0.35605633]]\n",
      "technologies [[0.38568747]]\n",
      "processes [[0.39017558]]\n",
      "proprietary [[0.42891064]]\n",
      "functional [[0.4320678]]\n",
      "efficacy [[0.43297523]]\n",
      "subjective [[0.48869294]]\n",
      "synthesis [[0.49266905]]\n",
      "vivo [[0.50209945]]\n",
      "Datafeed [[0.5150604]]\n",
      "ProQuote [[0.53543985]]\n",
      "olp [[0.5482096]]\n",
      "defaultHeight [[0.5899529]]\n",
      "pusporte [[0.60609496]]\n",
      "Nathesh [[0.6230533]]\n",
      "glycosylated [[0.6605201]]\n",
      "PittsburgMo [[0.6821924]]\n",
      "PittsburgKan [[0.70375466]]\n",
      "MookieTheCat [[0.7224848]]\n",
      "actual sim was [[0.29064178]] while avg was [[0.19018801]]\n"
     ]
    }
   ],
   "source": [
    "a,b = nn_word(\"compartmentalize\", data_dict, model.lstm, chardict, word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 2\n",
    "H = 100\n",
    "character_dim = 50\n",
    "output_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 50)            2800      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 200)           120800    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 424,700\n",
      "Trainable params: 424,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "760310/760310 [==============================] - 741s 975us/sample - loss: 0.0192\n",
      "Epoch 2/25\n",
      "760310/760310 [==============================] - 728s 958us/sample - loss: 0.0189\n",
      "Epoch 3/25\n",
      "760310/760310 [==============================] - 728s 957us/sample - loss: 0.0188\n",
      "Epoch 4/25\n",
      "760310/760310 [==============================] - 731s 962us/sample - loss: 0.0187\n",
      "Epoch 5/25\n",
      "760310/760310 [==============================] - 725s 953us/sample - loss: 0.0186\n",
      "Epoch 6/25\n",
      "760310/760310 [==============================] - 731s 961us/sample - loss: 0.0185\n",
      "Epoch 7/25\n",
      "760310/760310 [==============================] - 725s 954us/sample - loss: 0.0184\n",
      "Epoch 8/25\n",
      "760310/760310 [==============================] - 731s 961us/sample - loss: 0.0184\n",
      "Epoch 9/25\n",
      "760310/760310 [==============================] - 730s 960us/sample - loss: 0.0183\n",
      "Epoch 10/25\n",
      "760310/760310 [==============================] - 729s 959us/sample - loss: 0.0183\n",
      "Epoch 11/25\n",
      "760310/760310 [==============================] - 725s 953us/sample - loss: 0.0182\n",
      "Epoch 12/25\n",
      "760310/760310 [==============================] - 727s 957us/sample - loss: 0.0182\n",
      "Epoch 13/25\n",
      "760310/760310 [==============================] - 721s 948us/sample - loss: 0.0181\n",
      "Epoch 14/25\n",
      "760310/760310 [==============================] - 716s 942us/sample - loss: 0.0181\n",
      "Epoch 15/25\n",
      "760310/760310 [==============================] - 719s 946us/sample - loss: 0.0181\n",
      "Epoch 16/25\n",
      "760310/760310 [==============================] - 711s 936us/sample - loss: 0.0181\n",
      "Epoch 17/25\n",
      "760310/760310 [==============================] - 708s 931us/sample - loss: 0.0180\n",
      "Epoch 18/25\n",
      "760310/760310 [==============================] - 713s 938us/sample - loss: 0.0180\n",
      "Epoch 19/25\n",
      "760310/760310 [==============================] - 705s 927us/sample - loss: 0.0180\n",
      "Epoch 20/25\n",
      "760310/760310 [==============================] - 707s 930us/sample - loss: 0.0180\n",
      "Epoch 21/25\n",
      "760310/760310 [==============================] - 708s 931us/sample - loss: 0.0180\n",
      "Epoch 22/25\n",
      "760310/760310 [==============================] - 701s 922us/sample - loss: 0.0179\n",
      "Epoch 23/25\n",
      "760310/760310 [==============================] - 710s 933us/sample - loss: 0.0179\n",
      "Epoch 24/25\n",
      "760310/760310 [==============================] - 701s 922us/sample - loss: 0.0179\n",
      "Epoch 25/25\n",
      "760310/760310 [==============================] - 707s 931us/sample - loss: 0.0179\n"
     ]
    }
   ],
   "source": [
    "model_complex = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 25, full_set = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Save Succesful'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_model(\"./mimic_model_complex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sim is [[0.44117203]]\n",
      "in [[0.15407538]]\n",
      "for [[0.15527652]]\n",
      "that [[0.1856101]]\n",
      "on [[0.18662052]]\n",
      "the [[0.23123805]]\n",
      "not [[0.24033497]]\n",
      "their [[0.27377588]]\n",
      "you [[0.30320066]]\n",
      "all [[0.30395806]]\n",
      "do [[0.34623665]]\n",
      "just [[0.4128584]]\n",
      "getting [[0.47810704]]\n",
      "talking [[0.54108655]]\n",
      "putting [[0.54895616]]\n",
      "ripping [[0.6050508]]\n",
      "messing [[0.6262998]]\n",
      "actual sim was [[0.44117203]] while avg was [[0.11221445]]\n"
     ]
    }
   ],
   "source": [
    "a,b = nn_word(\"crying\", data_dict, model_complex.lstm, chardict, word2vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer after first bidirectional layers + dropout after final\n",
    "model_complex_attention = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 25, full_set = True, attention=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
