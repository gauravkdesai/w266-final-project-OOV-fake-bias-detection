{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things left to do on model front\n",
    "\n",
    "    1) Either use original polyglot embeddings or original code on word2vec to make sure our model is getting similar results; would be really dumb if we have a small mistake in our model and aren't actually training it correctly\n",
    "\n",
    "    2) Build 1-2 more models of increasing complexity.  Perhaps we add attention to one of the biLSTM layers or attempt encoder decoder approach.\n",
    "    \n",
    "    3) Build script that takes a feature and works through getting avg. embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Useful library for embeddings\n",
    "\n",
    "import class_lstm\n",
    "from class_lstm import MimicLSTM\n",
    "\n",
    "import word2vec_preprocess\n",
    "import word2vec_preprocess as preprocess\n",
    "\n",
    "import gensim\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing import sequence \n",
    "import tensorflow as tf\n",
    "\n",
    "import gensim\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './word2vec_model/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "data_dict, chardict = preprocess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_word(word, word_emb_dict, lstm_model, char_dict, word2vec):\n",
    "    indices = np.array([char_dict[letter] for letter in word])\n",
    "    indices = sequence.pad_sequences([indices], maxlen=25) \n",
    "    prediction = lstm_model.predict(tf.reshape(indices,(1,-1)), steps=1)\n",
    "    \n",
    "    if word in word2vec:\n",
    "        actual = np.reshape(word2vec[word],(1,-1))\n",
    "        actual_sim = cosine_similarity(actual,prediction)\n",
    "        print(\"Actual sim is %s\" %actual_sim)\n",
    "    else:\n",
    "        actual_sim = \"Not Known\"\n",
    "    \n",
    "    best_word = ''\n",
    "    best_sim = 0\n",
    "    total_sim = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for k,v in word_emb_dict.items():\n",
    "        \n",
    "        v = np.reshape(v,(1,-1))\n",
    "        sim = cosine_similarity(prediction, v)\n",
    "        total_sim += sim\n",
    "        total_count += 1\n",
    "        \n",
    "        if sim > best_sim:\n",
    "            best_word = k\n",
    "            best_sim = sim\n",
    "            print(best_word, best_sim)\n",
    "    avg_sim = total_sim/total_count\n",
    "    \n",
    "    print(\"actual sim was %s while avg was %s\" %(actual_sim,avg_sim))\n",
    "    return actual_sim, avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'word2vec_preprocess' from '/home/cgleach/w266/final_project/w266-final-project-OOV-fake-bias-detection/original_work/word2vec_preprocess.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload our packages as we test \n",
    "importlib.reload(class_lstm)\n",
    "importlib.reload(word2vec_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 20)            1120      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               28400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 59,820\n",
      "Trainable params: 59,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "760310/760310 [==============================] - 121s 159us/step - loss: 0.0193\n",
      "Epoch 2/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0190\n",
      "Epoch 3/30\n",
      "760310/760310 [==============================] - 116s 153us/step - loss: 0.0190\n",
      "Epoch 4/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0189\n",
      "Epoch 5/30\n",
      "760310/760310 [==============================] - 120s 157us/step - loss: 0.0189\n",
      "Epoch 6/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0188\n",
      "Epoch 7/30\n",
      "760310/760310 [==============================] - 117s 155us/step - loss: 0.0188\n",
      "Epoch 8/30\n",
      "760310/760310 [==============================] - 119s 156us/step - loss: 0.0187\n",
      "Epoch 9/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0187\n",
      "Epoch 10/30\n",
      "760310/760310 [==============================] - 117s 154us/step - loss: 0.0187\n",
      "Epoch 11/30\n",
      "760310/760310 [==============================] - 116s 153us/step - loss: 0.0187\n",
      "Epoch 12/30\n",
      "760310/760310 [==============================] - 118s 155us/step - loss: 0.0187\n",
      "Epoch 13/30\n",
      " 82000/760310 [==>...........................] - ETA: 1:44 - loss: 0.0186"
     ]
    }
   ],
   "source": [
    "# Original model as outlined in paper\n",
    "layers = 1\n",
    "H = 50\n",
    "character_dim = 20\n",
    "output_dim = 300\n",
    "\n",
    "model = MimicLSTM(layers, H, chardict, character_dim, data_dict, \n",
    "                  batch_size=1000, epochs=30, full_set=True)\n",
    "\n",
    "model.save_model(\"./mimic_model_original.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadmodel = MimicLSTM(layers, H, chardict, character_dim, data_dict, \n",
    "                  train=False, load_path=\"./mimic_model_original.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Double the layers\n",
    "layers = 2\n",
    "H = 50\n",
    "character_dim = 20\n",
    "output_dim = 300\n",
    "\n",
    "model_complex = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 30, full_set = True)\n",
    "\n",
    "model.save_model(\"./mimic_model_complex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 100)           5600      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 200)           160800    \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, 25, 200)           12865     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 480,365\n",
      "Trainable params: 480,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/cgleach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/30\n",
      "760310/760310 [==============================] - 882s 1ms/step - loss: 0.0193\n",
      "Epoch 2/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0190\n",
      "Epoch 3/30\n",
      "760310/760310 [==============================] - 871s 1ms/step - loss: 0.0189\n",
      "Epoch 4/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: 0.0187\n",
      "Epoch 5/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0186\n",
      "Epoch 6/30\n",
      "760310/760310 [==============================] - 870s 1ms/step - loss: 0.0185\n",
      "Epoch 7/30\n",
      "760310/760310 [==============================] - 879s 1ms/step - loss: 0.0184\n",
      "Epoch 8/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: 0.0184\n",
      "Epoch 9/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: 0.0183\n",
      "Epoch 10/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: 0.0182\n",
      "Epoch 11/30\n",
      "760310/760310 [==============================] - 877s 1ms/step - loss: 0.0182\n",
      "Epoch 12/30\n",
      "760310/760310 [==============================] - 874s 1ms/step - loss: 0.0182\n",
      "Epoch 13/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0181\n",
      "Epoch 14/30\n",
      "760310/760310 [==============================] - 872s 1ms/step - loss: 0.0181\n",
      "Epoch 15/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0181\n",
      "Epoch 16/30\n",
      "760310/760310 [==============================] - 878s 1ms/step - loss: 0.0180\n",
      "Epoch 17/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0180\n",
      "Epoch 18/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0180\n",
      "Epoch 19/30\n",
      "760310/760310 [==============================] - 876s 1ms/step - loss: 0.0180\n",
      "Epoch 20/30\n",
      "760310/760310 [==============================] - 874s 1ms/step - loss: 0.0179\n",
      "Epoch 21/30\n",
      "760310/760310 [==============================] - 873s 1ms/step - loss: 0.0179\n",
      "Epoch 22/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0179\n",
      "Epoch 23/30\n",
      "760310/760310 [==============================] - 871s 1ms/step - loss: 0.0179\n",
      "Epoch 24/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: 0.0179\n",
      "Epoch 25/30\n",
      "760310/760310 [==============================] - 875s 1ms/step - loss: 0.0179\n",
      "Epoch 26/30\n",
      "760310/760310 [==============================] - 879s 1ms/step - loss: 0.0178\n",
      "Epoch 27/30\n",
      "760310/760310 [==============================] - 866s 1ms/step - loss: 0.0178\n",
      "Epoch 28/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0178\n",
      "Epoch 29/30\n",
      "760310/760310 [==============================] - 868s 1ms/step - loss: 0.0178\n",
      "Epoch 30/30\n",
      "760310/760310 [==============================] - 867s 1ms/step - loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "# Super complicated model\n",
    "\n",
    "layers = 2\n",
    "H = 100\n",
    "character_dim = 100\n",
    "output_dim = 300\n",
    "recurrent_dropout = .05\n",
    "dense_dropout = .05\n",
    "attention = True\n",
    "\n",
    "model_complex_attention = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 30, full_set = True, use_attention=attention, \n",
    "                        recurrent_dropout = recurrent_dropout, dense_dropout = dense_dropout)\n",
    "\n",
    "#model_complex_attention.save_model(\"./mimic_model_complex_attention.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 100)           5600      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 25, 200)           160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               60300     \n",
      "=================================================================\n",
      "Total params: 467,500\n",
      "Trainable params: 467,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      " 19000/760310 [..............................] - ETA: 18:06 - loss: 0.7247"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d9de87450b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m model_complex_attention = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n\u001b[1;32m     10\u001b[0m                           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         recurrent_dropout = recurrent_dropout, dense_dropout = dense_dropout)\n\u001b[0m",
      "\u001b[0;32m~/w266/final_project/w266-final-project-OOV-fake-bias-detection/original_work/class_lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, H, chardict, character_dim, data_dictionary, epochs, batch_size, optimizer, loss_function, train, full_set, load_path, use_attention, recurrent_dropout, dense_dropout)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/w266/final_project/w266-final-project-OOV-fake-bias-detection/original_work/class_lstm.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epochs, batch_size, full_set)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfull_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             self.lstm.fit(self.train_w, self.train_e, validation_data = (self.test_w, self.test_e),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers = 2\n",
    "H = 100\n",
    "character_dim = 100\n",
    "output_dim = 300\n",
    "recurrent_dropout = .05\n",
    "dense_dropout = .05\n",
    "attention = True\n",
    "\n",
    "model_complex_attention_cosine = MimicLSTM(layers, H, chardict, character_dim, data_dict, batch_size=1000, \n",
    "                          epochs = 30, full_set = True, use_attention=attention, \n",
    "                        recurrent_dropout = recurrent_dropout, dense_dropout = dense_dropout)\n",
    "\n",
    "model_complex_attention_cosine.save_model(\"./mimic_model_complex_attention_cosine_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
