{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.549946129322052),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick sanity check that our model makes sense and is loaded correctly\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See list of all characters google includes in their word2vec model.  We \n",
    "# will not support most of these characters as they are extremely unlikely to occur\n",
    "# on wikipedia pages\n",
    "\n",
    "char_dict = dict({})\n",
    "for idx, key in enumerate(model.wv.vocab):\n",
    "    for char in key:\n",
    "        char_dict[char] = 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of rules for words\n",
    "\n",
    "1) We will remove quotations and commas\n",
    "2) We will support -,_,',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Useful library for embeddings\n",
    "import gensim\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import sequence \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Embedding, Dense, LSTM, Bidirectional\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model from our thing\n",
    "# Note: DONT COMMIT THAT FILE TO GITHUB!!!\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './word2vec_model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with upper and lower case letters and associated index\n",
    "# Note: We include underscores, hyphens, and apostrophes but ignore other characters\n",
    "# found in word2vec model, including chinese symbols, emojis, etc\n",
    "lower_case_letter_dict = {letter: int(index)+1 for index, letter in enumerate(ascii_lowercase, start=1)} \n",
    "upper_case_letter_dict = {letter: int(index)+27 for index, letter in enumerate(ascii_uppercase, start=1)} \n",
    "chardict = {**lower_case_letter_dict, **upper_case_letter_dict}\n",
    "chardict['_']=54\n",
    "chardict['-']=55\n",
    "chardict['\\'']=56\n",
    "\n",
    "reverse_chardict = {}\n",
    "for k,v in chardict.items():\n",
    "    reverse_chardict[v] = k\n",
    "\n",
    "def include_word(word, chardict):\n",
    "    \"\"\"\n",
    "    Function to determine if word can be included and perform any parsing\n",
    "    \"\"\"\n",
    "    if all(char in chardict.keys() for char in word):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# Create list of words which will be used in training/testing our model\n",
    "all_words = dict({})\n",
    "\n",
    "# For every word in word2vec model establish if it is \"allowed\"; if it is\n",
    "# add the word to our all_words dict, with the embedding as the value\n",
    "for idx, key in enumerate(model.wv.vocab):\n",
    "    if include_word(key, chardict):\n",
    "        all_words[key] = model.wv[key]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Create two lists corresponding to X & Y \n",
    "words = []\n",
    "embeddings = []\n",
    "\n",
    "for k,v in all_words.items():\n",
    "    words.append(k)\n",
    "    embeddings.append(v)\n",
    "\n",
    "# Convert characters to index references and all lists to numpy arrays\n",
    "words_index = [[chardict[char] for char in word] for word in words]\n",
    "words_index = np.array(words_index)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Establish train/test splits \n",
    "train_words, test_words, train_embeddings, test_embeddings = train_test_split(words_index, embeddings, \n",
    "                                                                              test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    pass\n",
    "# code from class utils library below; need to do something similar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def rnnlm_batch_generator(ids, batch_size, max_time):\n",
    "#     \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
    "#     # Clip to multiple of max_time for convenience\n",
    "#     clip_len = ((len(ids)-1) // batch_size) * batch_size\n",
    "#     input_w = ids[:clip_len]     # current word\n",
    "#     target_y = ids[1:clip_len+1]  # next word\n",
    "#     # Reshape so we can select columns\n",
    "#     input_w = input_w.reshape([batch_size,-1])\n",
    "#     target_y = target_y.reshape([batch_size,-1])\n",
    "\n",
    "#     # Yield batches\n",
    "#     for i in range(0, input_w.shape[1], max_time):\n",
    "#         yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
    "\n",
    "        \n",
    "        \n",
    "# for i, (w,y) in enumerate(rnnlm_batch_generator([3,4,3],2,1)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0, 42, 13, 26, 14, 17, 10, 18, 22,  6, 54, 39,\n",
       "       26, 16, 15], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad each sequence with zeroes if it is less than 20.\n",
    "X_train = sequence.pad_sequences(train_words, maxlen=20) \n",
    "X_test = sequence.pad_sequences(test_words, maxlen=20) \n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train[:10000]\n",
    "emb_test = train_embeddings[:10000]\n",
    "\n",
    "val_x = X_train[10000:14000]\n",
    "val_y = train_embeddings[10000:14000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 4000 samples\n",
      "10000/10000 [==============================] - 66s 7ms/sample - loss: 0.0164 - val_loss: 0.0160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b85984748>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "# Presumably this part works; takes in batch-size and integer indices and creates embedding layer\n",
    "# with 300d embeddings\n",
    "lstm.add(Embedding(57, 300))\n",
    "\n",
    "# Zero idea if this part is doing what we want....might have to deviate from Keras and define things manually\n",
    "# or define custom loss function.  N\n",
    "lstm.add(Bidirectional(LSTM(300, return_sequences=False)))\n",
    "lstm.add(Dense(300, activation='linear'))\n",
    "lstm.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "lstm.fit(test,emb_test, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way to do it in tensorflow for when we go to actually train this thing\n",
    "\n",
    "\n",
    "# mapping_strings = tf.constant([k for k,v in combined.items()])\n",
    "# table = tf.contrib.lookup.index_table_from_tensor(\n",
    "#     mapping=mapping_strings, num_oov_buckets=1, default_value=-1)\n",
    "# tf.tables_initializer().run()\n",
    "\n",
    "# features = tf.constant(['a','b','c','_'])\n",
    "# ids = table.lookup(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dynamic shape info from inputs\n",
    "with tf.name_scope(\"batch_size\"):\n",
    "    self.batch_size_ = tf.shape(self.input_w_)[0]\n",
    "with tf.name_scope(\"max_time\"):\n",
    "    self.max_time_ = tf.shape(self.input_w_)[1]\n",
    "\n",
    "with tf.name_scope(\"training_inputs\")\n",
    "    # input_w will be batch_size by max_time, where max_time is max letters we allow\n",
    "    self.input_w_ = tf.placeholder(tf.int32, [None, None], name=\"w\")\n",
    "    # Target_y will be batch_size by embedding size\n",
    "    self.target_y_ = tf.placeholder(tf.int32, [None, None], name=\"y\")\n",
    "\n",
    "# Construct embedding layer\n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    # Assume a 300d vector for each character; will replace with dynamic variables\n",
    "    self.W_in_ = tf.get_variable(\"W_in_\", [55,300], dtype=tf.float32, \n",
    "        initializer=tf.random_uniform_initializer(-1,1), trainable=True)\n",
    "\n",
    "    self.xs_ = tf.nn.embedding_lookup(self.W_in_, self.input_w_)\n",
    "\n",
    "with tf.name_scope(\"model_creation\"):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSMT(300)))\n",
    "\n",
    "\n",
    "#     model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5,\n",
    "#     10)))\n",
    "#     model.add(Bidirectional(LSTM(10)))\n",
    "#     model.add(Dense(5))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Decisions to prepare training data\n",
    "\n",
    "1) How do we handle capitalization, and comined words like \"crown_prince\" in above example?\n",
    "2) How many words do we want to include?  Top x, random x, etc?\n",
    "\n",
    "## Key Actions to prepare training data\n",
    "\n",
    "1) Write function to create training vectors which are series of character indices for features, \n",
    "   and embedding for target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of LSTM Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible useful references\n",
    "\n",
    "https://www.tensorflow.org/guide/keras\n",
    "https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n",
    "https://keras.io/getting-started/sequential-model-guide/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
