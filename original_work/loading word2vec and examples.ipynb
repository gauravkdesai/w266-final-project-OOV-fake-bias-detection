{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.549946129322052),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick sanity check that our model makes sense and is loaded correctly\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See list of all characters google includes in their word2vec model.  We \n",
    "# will not support most of these characters as they are extremely unlikely to occur\n",
    "# on wikipedia pages\n",
    "\n",
    "char_dict = dict({})\n",
    "for idx, key in enumerate(model.wv.vocab):\n",
    "    for char in key:\n",
    "        char_dict[char] = 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of rules for words\n",
    "\n",
    "1) We will remove quotations and commas\n",
    "2) We will support -,_,',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Useful library for embeddings\n",
    "import gensim\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import sequence \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Embedding, Dense, LSTM, Bidirectional\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model from our thing\n",
    "# Note: DONT COMMIT THAT FILE TO GITHUB!!!\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './word2vec_model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/cgleach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with upper and lower case letters and associated index\n",
    "# Note: We include underscores, hyphens, and apostrophes but ignore other characters\n",
    "# found in word2vec model, including chinese symbols, emojis, etc\n",
    "lower_case_letter_dict = {letter: int(index)+1 for index, letter in enumerate(ascii_lowercase, start=1)} \n",
    "upper_case_letter_dict = {letter: int(index)+27 for index, letter in enumerate(ascii_uppercase, start=1)} \n",
    "chardict = {**lower_case_letter_dict, **upper_case_letter_dict}\n",
    "chardict['_']=54\n",
    "chardict['-']=55\n",
    "chardict['\\'']=56\n",
    "chardict['.']=57\n",
    "\n",
    "reverse_chardict = {}\n",
    "for k,v in chardict.items():\n",
    "    reverse_chardict[v] = k\n",
    "\n",
    "def include_word(word, chardict):\n",
    "    \"\"\"\n",
    "    Function to determine if word can be included and perform any parsing\n",
    "    \"\"\"\n",
    "    if all(char in chardict.keys() for char in word):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# Create list of words which will be used in training/testing our model\n",
    "all_words = dict({})\n",
    "\n",
    "# For every word in word2vec model establish if it is \"allowed\"; if it is\n",
    "# add the word to our all_words dict, with the embedding as the value\n",
    "for idx, key in enumerate(model.wv.vocab):\n",
    "    if include_word(key, chardict):\n",
    "        all_words[key] = model.wv[key]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Create two lists corresponding to X & Y \n",
    "words = []\n",
    "embeddings = []\n",
    "\n",
    "for k,v in all_words.items():\n",
    "    words.append(k)\n",
    "    embeddings.append(v)\n",
    "\n",
    "# Convert characters to index references and all lists to numpy arrays\n",
    "words_index = [[chardict[char] for char in word] for word in words]\n",
    "words_index = np.array(words_index)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Establish train/test splits \n",
    "train_words, test_words, train_embeddings, test_embeddings = train_test_split(words_index, embeddings, \n",
    "                                                                              test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    pass\n",
    "# code from class utils library below; need to do something similar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def rnnlm_batch_generator(ids, batch_size, max_time):\n",
    "#     \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
    "#     # Clip to multiple of max_time for convenience\n",
    "#     clip_len = ((len(ids)-1) // batch_size) * batch_size\n",
    "#     input_w = ids[:clip_len]     # current word\n",
    "#     target_y = ids[1:clip_len+1]  # next word\n",
    "#     # Reshape so we can select columns\n",
    "#     input_w = input_w.reshape([batch_size,-1])\n",
    "#     target_y = target_y.reshape([batch_size,-1])\n",
    "\n",
    "#     # Yield batches\n",
    "#     for i in range(0, input_w.shape[1], max_time):\n",
    "#         yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
    "\n",
    "        \n",
    "        \n",
    "# for i, (w,y) in enumerate(rnnlm_batch_generator([3,4,3],2,1)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2572551, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 36,\n",
       "       45, 29, 20], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad each sequence with zeroes if it is less than 20.\n",
    "X_train = sequence.pad_sequences(train_words, maxlen=20) \n",
    "X_test = sequence.pad_sequences(test_words, maxlen=20) \n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train[:100000]\n",
    "emb_test = train_embeddings[:100000]\n",
    "\n",
    "val_x = X_train[100000:140000]\n",
    "val_y = train_embeddings[100000:140000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2561681 ,  0.0440715 , -0.01946591,  0.5048307 , -1.9254456 ,\n",
       "        -0.40805894, -0.01643568, -1.8172401 ,  0.7498343 , -0.3580538 ,\n",
       "        -0.8683603 , -1.385504  , -2.882206  , -0.38658762,  3.2857318 ,\n",
       "         0.3188932 , -1.8016087 ,  2.9225051 , -0.4670478 , -0.78816897,\n",
       "         0.05356523, -0.68285334, -3.4449804 , -0.25026512, -2.0927777 ,\n",
       "         0.32961357, -0.9435488 ,  2.9287925 , -1.8356545 , -0.8652038 ,\n",
       "         0.26002634, -0.601628  ,  0.8806363 , -0.32313663,  1.133319  ,\n",
       "         1.2267165 , -1.112298  ,  0.45222613, -3.3475816 , -1.4514046 ,\n",
       "         2.2781868 ,  1.2351414 , -1.0076942 , -2.2708905 , -0.18919292,\n",
       "         1.1437875 , -0.06780577, -0.75346076, -3.1618314 ,  0.7291281 ,\n",
       "         0.55477077,  2.7325916 ,  2.4650793 , -0.08224475,  1.7880992 ,\n",
       "        -1.206544  ,  2.094314  , -0.16124809, -0.61718667, -1.6677612 ,\n",
       "         0.45440984, -0.70312715,  1.4425989 ,  0.7693058 , -0.12254685,\n",
       "         0.14916909, -0.01980835,  2.7577844 , -0.2988838 , -0.48172045,\n",
       "        -0.7280458 ,  1.2959348 , -0.25924578, -1.099468  ,  1.8757688 ,\n",
       "        -2.193966  ,  1.0898765 ,  2.3133016 , -1.3976734 ,  0.76160854,\n",
       "         2.1486373 ,  1.1507294 , -2.8349895 ,  2.1388643 ,  0.10887694,\n",
       "        -1.4622748 , -0.453924  ,  1.1644934 , -1.399396  , -0.86516726,\n",
       "        -0.30936334, -0.30445182,  0.71132165,  1.1018145 ,  0.8407512 ,\n",
       "         2.7417645 , -1.0663108 , -0.28699517,  1.6386595 ,  0.48521185,\n",
       "        -0.8921777 , -0.38009274,  0.4709729 ,  1.1908895 ,  0.54856634,\n",
       "         0.5072009 , -0.03751922,  1.271303  , -2.8116493 ,  0.1393429 ,\n",
       "         0.15106106,  0.39472395,  0.21124893,  2.6334639 , -1.7322092 ,\n",
       "         0.19605505,  3.103739  ,  3.2620811 , -1.5448921 ,  0.6552019 ,\n",
       "        -0.4741482 , -1.6598457 , -0.6073878 ,  0.50417566, -1.2211522 ,\n",
       "         1.636055  , -0.41377628,  1.0930287 , -2.9868066 ,  1.4193639 ,\n",
       "        -0.87450325, -0.56538963, -0.74797237,  0.06548147, -1.1100534 ,\n",
       "         2.530198  ,  0.3211999 , -2.6168084 ,  0.2767563 , -1.2919289 ,\n",
       "        -0.6201159 , -0.43629587, -0.23380089,  2.3044734 ,  1.2720746 ,\n",
       "         1.4228207 ,  0.04491958,  1.1527193 ,  0.7092477 ,  1.4044776 ,\n",
       "         2.298758  , -1.1349983 , -1.5498229 , -0.70788026,  0.49813807,\n",
       "        -0.06563568, -0.6001247 , -2.2793627 ,  1.1694593 , -2.4546385 ,\n",
       "        -1.6816249 ,  1.922802  ,  1.3461071 ,  1.3699245 ,  0.6645497 ,\n",
       "         0.7490357 ,  0.2693252 ,  0.674475  ,  0.2237772 , -1.5564187 ,\n",
       "         0.3315446 , -0.7780513 ,  1.3312235 , -0.39259487,  0.9688796 ,\n",
       "         0.5002211 , -2.0985425 ,  0.52089095, -0.45020866,  0.5215156 ,\n",
       "         1.1315205 ,  0.85756516, -4.0829005 ,  1.8106465 ,  1.5384079 ,\n",
       "         1.170189  , -0.55983585,  0.20167917,  0.35711408,  0.52938724,\n",
       "         0.44531834,  0.1044988 ,  1.6129946 , -1.4787649 , -0.5256473 ,\n",
       "        -0.73811895,  1.6512332 , -1.0791478 ,  2.3550026 , -0.8127022 ,\n",
       "         0.41299623,  0.44372404, -0.65726924,  1.7967045 , -1.5492581 ,\n",
       "         0.8082534 ,  1.7735054 , -0.09511971, -1.9588652 ,  3.514772  ,\n",
       "         0.0814712 ,  2.6668081 ,  1.004936  , -0.09733647,  1.9600805 ,\n",
       "         2.5708656 ,  0.02321973,  1.2521029 , -0.23134357, -0.74753135,\n",
       "         0.07148147, -2.9110107 , -2.0474138 ,  2.3949473 ,  1.2938967 ,\n",
       "         0.63198876, -0.06060675,  0.26182103, -5.554685  ,  3.0630603 ,\n",
       "         0.8322129 , -0.20039988, -4.0912266 , -1.7888672 , -1.5799022 ,\n",
       "        -2.310739  , -2.6463554 , -2.948865  , -0.47212273,  1.8905253 ,\n",
       "         1.0174499 ,  0.03779465, -1.0601634 , -0.03492916, -1.3375494 ,\n",
       "         1.7096045 , -2.31282   , -1.4505401 , -0.37371606,  0.22424173,\n",
       "         0.9106376 ,  1.9669166 , -2.2033646 ,  0.5535267 , -1.4349725 ,\n",
       "         0.2626645 , -0.8073666 , -0.6114195 , -1.9392948 , -0.52225024,\n",
       "         2.3017387 ,  2.7051735 ,  0.42138726,  0.37527806,  0.96713996,\n",
       "         1.3524699 , -0.46159077,  1.5124259 , -0.52184594,  0.8703506 ,\n",
       "         1.6423707 , -1.3277056 ,  0.9272743 ,  0.3641696 , -0.3224368 ,\n",
       "        -2.3626814 ,  1.1393871 , -0.8030617 , -0.42946577, -2.026225  ,\n",
       "         0.10177833, -1.9017534 , -1.6550671 ,  0.26361883, -0.35224092,\n",
       "         1.4870064 ,  2.3283503 , -1.8459747 ,  0.32347623, -0.7536168 ,\n",
       "         0.8875295 ,  1.2445818 ,  1.7206094 , -1.3510691 ,  0.42525232,\n",
       "        -2.2492366 , -0.46682554,  1.5987926 , -0.67511415,  0.24771512]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes: This is if we do it straight tensorflow style.  I think I just have to make the loss\n",
    "# at the end correct, rather than just testing a subtraction\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "tf.reset_default_graph()\n",
    "# Define lstm cells with tensorflow\n",
    "# Forward direction cell\n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    # Assume a 300d vector for each character; will replace with dynamic variables\n",
    "    W_in_ = tf.get_variable(\"W_in_\", [58,300], dtype=tf.float32, \n",
    "            initializer=tf.random_uniform_initializer(-1,1), trainable=True)\n",
    "\n",
    "    xs_ = tf.reshape(tf.nn.embedding_lookup(W_in_, test[0]),(1,20,300))\n",
    "    x = tf.unstack(xs_, 20, 1)\n",
    "\n",
    "W_out_ = tf.get_variable(\"W_out_\", [600,300], dtype=tf.float32, \n",
    "            initializer=tf.random_uniform_initializer(-1,1), trainable=True)\n",
    "b_out_ = tf.get_variable(\"b_out_\", (1,300), dtype=tf.float32, \n",
    "            initializer=tf.random_uniform_initializer(-1,1), trainable=True)\n",
    "\n",
    "lstm_fw_cell = rnn.BasicLSTMCell(300, forget_bias=1.0)\n",
    "# Backward direction cell\n",
    "lstm_bw_cell = rnn.BasicLSTMCell(300, forget_bias=1.0)\n",
    "\n",
    "# Get lstm cell output\n",
    "outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                          dtype=tf.float32)\n",
    "\n",
    "final_out = tf.add(tf.matmul(outputs[-1], W_out_), b_out_)\n",
    "\n",
    "loss = final_out-emb_test[0]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff I've learned so far\n",
    "\n",
    "    1) Evertyhing keras must be imported as tensorflow.keras otherwise you're using different versions\n",
    "    2) Keras is really sweet but I have no idea if its working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08886719,  0.15625   ,  0.19726562,  0.33984375, -0.09130859,\n",
       "       -0.2578125 ,  0.11523438, -0.02355957,  0.09863281, -0.01385498,\n",
       "        0.35351562, -0.34375   , -0.1953125 , -0.02697754, -0.4921875 ,\n",
       "        0.20605469, -0.06689453,  0.09228516,  0.04223633,  0.25195312,\n",
       "        0.13574219,  0.03979492, -0.1875    , -0.0234375 ,  0.19726562,\n",
       "       -0.23925781, -0.19335938,  0.12988281, -0.18847656, -0.16015625,\n",
       "        0.18652344, -0.11328125, -0.02526855, -0.10351562,  0.02905273,\n",
       "       -0.125     ,  0.02087402,  0.26757812,  0.22558594, -0.21582031,\n",
       "        0.00442505,  0.265625  , -0.11279297, -0.11914062, -0.18554688,\n",
       "       -0.3671875 ,  0.31835938, -0.20898438, -0.15234375, -0.01879883,\n",
       "        0.06445312, -0.09472656,  0.07324219, -0.3046875 , -0.09814453,\n",
       "       -0.09765625,  0.2734375 ,  0.12695312, -0.02490234, -0.25390625,\n",
       "       -0.453125  , -0.08251953, -0.00300598, -0.44140625, -0.12011719,\n",
       "       -0.1171875 , -0.359375  ,  0.22265625, -0.16601562,  0.05029297,\n",
       "       -0.32421875, -0.23632812,  0.11962891, -0.36523438,  0.09423828,\n",
       "       -0.0859375 , -0.21582031, -0.55859375, -0.02990723,  0.21289062,\n",
       "       -0.18847656,  0.16699219, -0.03039551,  0.20898438, -0.203125  ,\n",
       "       -0.06445312,  0.01806641,  0.07666016,  0.03149414,  0.24511719,\n",
       "       -0.13085938,  0.07470703,  0.20214844, -0.10253906,  0.47265625,\n",
       "       -0.11425781,  0.2890625 ,  0.00163269, -0.0032959 , -0.08886719,\n",
       "        0.35546875,  0.0014267 ,  0.13476562, -0.359375  ,  0.33789062,\n",
       "       -0.09082031, -0.40625   , -0.23828125,  0.20898438,  0.34570312,\n",
       "       -0.09423828,  0.16210938,  0.11328125, -0.04345703,  0.14941406,\n",
       "       -0.17382812,  0.125     ,  0.07861328,  0.11425781, -0.06689453,\n",
       "       -0.01208496, -0.22265625, -0.20507812,  0.1875    ,  0.265625  ,\n",
       "       -0.05078125, -0.05102539, -0.06982422,  0.00430298,  0.05297852,\n",
       "        0.04174805, -0.16601562, -0.11572266, -0.22265625, -0.19238281,\n",
       "       -0.03222656, -0.03881836, -0.26171875, -0.00866699, -0.09228516,\n",
       "       -0.1484375 , -0.17871094,  0.05932617, -0.14746094, -0.22460938,\n",
       "       -0.02087402, -0.27734375,  0.08789062,  0.10742188,  0.02416992,\n",
       "       -0.27929688, -0.4609375 , -0.34765625, -0.09521484,  0.01879883,\n",
       "        0.06591797, -0.06079102, -0.18457031,  0.07324219,  0.03637695,\n",
       "        0.22363281,  0.14257812,  0.00279236,  0.06982422, -0.11035156,\n",
       "       -0.05371094,  0.29296875,  0.06933594,  0.00964355,  0.18554688,\n",
       "        0.1796875 , -0.29492188,  0.18847656, -0.10839844,  0.3125    ,\n",
       "       -0.28710938, -0.02368164, -0.06005859, -0.23046875,  0.01422119,\n",
       "        0.05737305, -0.07763672,  0.01574707, -0.04638672, -0.14648438,\n",
       "       -0.1640625 ,  0.1484375 ,  0.14550781,  0.2578125 , -0.13085938,\n",
       "       -0.26367188,  0.01977539, -0.16894531, -0.19824219, -0.03393555,\n",
       "       -0.10449219,  0.36914062, -0.18359375,  0.27148438, -0.07177734,\n",
       "       -0.0859375 ,  0.43164062, -0.02819824,  0.18066406,  0.01806641,\n",
       "        0.02709961,  0.07421875, -0.48828125,  0.265625  , -0.05517578,\n",
       "        0.0703125 ,  0.12060547, -0.05126953,  0.0213623 , -0.21191406,\n",
       "       -0.3671875 , -0.13964844,  0.28125   , -0.17871094, -0.421875  ,\n",
       "       -0.04394531,  0.21386719, -0.01953125, -0.09960938, -0.08300781,\n",
       "       -0.10693359,  0.04907227, -0.03564453,  0.09570312, -0.11865234,\n",
       "       -0.09228516,  0.14453125,  0.02490234, -0.16015625, -0.35546875,\n",
       "       -0.05810547,  0.41015625,  0.00515747,  0.203125  , -0.10644531,\n",
       "        0.3046875 , -0.36132812, -0.01660156,  0.14453125, -0.05004883,\n",
       "       -0.06396484, -0.12695312,  0.17773438, -0.13183594,  0.05493164,\n",
       "       -0.17773438, -0.40429688,  0.49023438, -0.26171875, -0.15722656,\n",
       "       -0.12695312,  0.2890625 ,  0.11621094,  0.02978516,  0.0625    ,\n",
       "        0.02441406, -0.23925781, -0.17773438,  0.13085938, -0.11914062,\n",
       "        0.04394531,  0.0246582 , -0.27539062, -0.3203125 , -0.08300781,\n",
       "        0.03088379,  0.01361084, -0.07080078,  0.24414062, -0.16503906,\n",
       "       -0.22851562, -0.19921875,  0.24023438,  0.14648438, -0.04101562,\n",
       "       -0.20898438, -0.203125  ,  0.13085938,  0.00689697,  0.40234375,\n",
       "        0.17382812,  0.14550781, -0.00344849,  0.38476562,  0.29492188,\n",
       "        0.04272461,  0.13964844, -0.27539062,  0.3203125 , -0.04736328,\n",
       "        0.14648438, -0.27148438, -0.09472656, -0.16015625, -0.12060547],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 20, 50)            2900      \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 100)               40400     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 73,600\n",
      "Trainable params: 73,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100000 samples, validate on 40000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 30s 296us/sample - loss: 0.0162 - val_loss: 0.0159\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 15s 152us/sample - loss: 0.0160 - val_loss: 0.0158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf74da5a90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes: This is Keras way of doing thinsg which if we can figure it out \n",
    "# is way easier, and also way more likely to help us in professional setting\n",
    "# This all runs, but I'm not 100% sure its doing what we want as the loss\n",
    "# really isn't going anywhere.\n",
    "\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(58, 50, input_length=20, embeddings_initializer=initializers.RandomUniform(-.2,.2)))\n",
    "#lstm.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "lstm.add(Bidirectional(LSTM(50, return_sequences=False)))\n",
    "lstm.add(Dense(300, activation='tanh'))\n",
    "#lstm.add(Dense(300,activation='linear'))\n",
    "lstm.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
    "lstm.summary()\n",
    "lstm.fit(test,emb_test, batch_size=1000, validation_data=(val_x, val_y), epochs=2)\n",
    "\n",
    "\n",
    "# Loss after 10k .0167\n",
    "# Loss after 25k .0163\n",
    "# Loss after 50k .0161\n",
    "# Loss after 75k .0160\n",
    "# Loss after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lstm.predict(tf.reshape(X_train[1:3],(2,20)),steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.037851535\n",
      "-0.23046875\n"
     ]
    }
   ],
   "source": [
    "print(a[0][25])\n",
    "print(train_embeddings[1][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way to do it in tensorflow for when we go to actually train this thing\n",
    "\n",
    "\n",
    "# mapping_strings = tf.constant([k for k,v in combined.items()])\n",
    "# table = tf.contrib.lookup.index_table_from_tensor(\n",
    "#     mapping=mapping_strings, num_oov_buckets=1, default_value=-1)\n",
    "# tf.tables_initializer().run()\n",
    "\n",
    "# features = tf.constant(['a','b','c','_'])\n",
    "# ids = table.lookup(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dynamic shape info from inputs\n",
    "with tf.name_scope(\"batch_size\"):\n",
    "    self.batch_size_ = tf.shape(self.input_w_)[0]\n",
    "with tf.name_scope(\"max_time\"):\n",
    "    self.max_time_ = tf.shape(self.input_w_)[1]\n",
    "\n",
    "with tf.name_scope(\"training_inputs\")\n",
    "    # input_w will be batch_size by max_time, where max_time is max letters we allow\n",
    "    self.input_w_ = tf.placeholder(tf.int32, [None, None], name=\"w\")\n",
    "    # Target_y will be batch_size by embedding size\n",
    "    self.target_y_ = tf.placeholder(tf.int32, [None, None], name=\"y\")\n",
    "\n",
    "# Construct embedding layer\n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    # Assume a 300d vector for each character; will replace with dynamic variables\n",
    "    self.W_in_ = tf.get_variable(\"W_in_\", [57,300], dtype=tf.float32, \n",
    "        initializer=tf.random_uniform_initializer(-1,1), trainable=True)\n",
    "\n",
    "    self.xs_ = tf.nn.embedding_lookup(self.W_in_, self.input_w_)\n",
    "\n",
    "with tf.name_scope(\"model_creation\"):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSMT(300)))\n",
    "\n",
    "\n",
    "#     model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5,\n",
    "#     10)))\n",
    "#     model.add(Bidirectional(LSTM(10)))\n",
    "#     model.add(Dense(5))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Decisions to prepare training data\n",
    "\n",
    "1) How do we handle capitalization, and comined words like \"crown_prince\" in above example?\n",
    "2) How many words do we want to include?  Top x, random x, etc?\n",
    "\n",
    "## Key Actions to prepare training data\n",
    "\n",
    "1) Write function to create training vectors which are series of character indices for features, \n",
    "   and embedding for target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of LSTM Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible useful references\n",
    "\n",
    "https://www.tensorflow.org/guide/keras\n",
    "https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n",
    "https://keras.io/getting-started/sequential-model-guide/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
